# Content Analyzer: Violence, Vulgar, and Child Labour Detection

Millions of people use various platforms to communicate, share information, and interact with others, becoming an essential part of our lives. However, incidents of violence, vulgarity, and child labor do occur on these platforms. Monitoring and identifying these incidents are crucial to prevent and reduce their occurrence. The Content Analyzer: Violence, Vulgar, and Child Labour Detection tool is designed to address this challenge.

## Overview

The Content Analyzer: Violence, Vulgar, and Child Labour Detection is a machine learning-based tool that recognizes and classifies images and videos into various categories, including child labor, violence, vulgar, and clean. It utilizes three models for prediction: the MainModel with four labels, the SecondModel with three labels, and the ThirdModel with three labels. The tool is powered by Python and can accurately classify media based on its content.

## Features

- Recognize and classify images and videos into categories such as child labor, violence, vulgar, and clean.
- Utilize machine learning models for accurate prediction and classification.
- Backend built with Flask microframework for handling requests and processing data.
- Frontend integrated with Angular framework for a user-friendly interface.
- Visualize data and insights using the d3.js framework.
- Store and manage data using the Postgres SQL database.

## Installation

To install and set up the Content Analyzer: Violence, Vulgar, and Child Labour Detection, follow these steps:

1. Clone the repository: `git clone https://github.com/your-username/repository-name.git`
2. Install the required dependencies by navigating to the project directory and running: `pip install -r requirements.txt`
3. Set up the database by following the instructions in the `database-setup.md` file.
4. Launch the backend server by running: `python app.py`
5. Open the frontend by navigating to the provided URL in your web browser.

## Usage

1. Upload or provide the URL of an image or video you want to analyze.
2. The tool will process the media and classify it into relevant categories (child labor, violence, vulgar, or clean).
3. View the results and insights generated by the tool, including visualizations where applicable.
4. Take necessary actions based on the identified content, such as reporting or moderation.

## Contributing

Contributions to the Content Analyzer: Violence, Vulgar, and Child Labour Detection are welcome! If you want to contribute, please follow these steps:

1. Fork the repository.
2. Create a new branch for your feature: `git checkout -b feature-name`
3. Make your changes and commit them: `git commit -m 'Add your message here'`
4. Push the changes to your forked repository: `git push origin feature-name`
5. Submit a pull request detailing your changes.


## Contact

If you have any questions, feedback, or inquiries, please contact moh.momin043@gmail.com or mohammadtaimur5920@gmail.com. We appreciate your interest and support!

